{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkW8_A6Tl-PF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42749ae-fc62-468d-f3ed-d6982d78a39a"
      },
      "source": [
        "!pip3 install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcks6_Z0mUjs"
      },
      "source": [
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "from PPO import PPO\n",
        "\n",
        "class PpoLearning():\n",
        "\n",
        "    def __init__(self, no_games, PPO):\n",
        "\n",
        "        self.no_games = no_games\n",
        "        self.ppo = PPO\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        game_rew_hist = [] # list to hold the total rewards per game\n",
        "        iters = 0\n",
        "        step = 0\n",
        "\n",
        "        for game in range(1, no_games+1):\n",
        "\n",
        "            done = False\n",
        "            game_total_rew = 0\n",
        "            state = env.reset()\n",
        "\n",
        "            while not done:\n",
        "                action, log_prob, value = self.ppo.get_action_value(state)\n",
        "                next_state , reward, done, info = env.step(action)\n",
        "                step += 1\n",
        "                game_total_rew += reward\n",
        "                self.ppo.memory.save_memory(state, value, action, log_prob, reward, done)\n",
        "                if step % self.ppo.memory.T == 0:\n",
        "                    self.ppo.optimize()\n",
        "                    iters += 1\n",
        "                    self.ppo.memory.reset()\n",
        "                state = next_state\n",
        "\n",
        "            game_rew_hist.append(game_total_rew)\n",
        "\n",
        "            if (game) % 10 == 0:\n",
        "                avg_score = np.mean(game_rew_hist[-10:])\n",
        "\n",
        "                print('Episode: ', game, 'average score:', avg_score, \n",
        "                'learning_iterations:', iters)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjBDBAOzmlBz",
        "outputId": "4357ec93-8aad-4e4f-ac7d-9e41e4880350"
      },
      "source": [
        "# define the environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "# hyper-parameters for PPO learning\n",
        "epochs = 4 # number of epochs per learning\n",
        "no_batches = 2 # number of batches for splitting the timesteps\n",
        "hidden_dim = 256\n",
        "gamma = 0.99 # discount factor\n",
        "gae_lambda = 0.95 # gae smoothing parameter\n",
        "lr_actor = 0.0003\n",
        "lr_critic = 0.0005\n",
        "clip = 0.2 # PPO clipping epsilon parameter\n",
        "T = 10 # timesteps per each learning\n",
        "\n",
        "ppo = PPO(env=env, T=T, hidden_dim=hidden_dim, gamma=gamma, \n",
        "    gae_lambda=gae_lambda, clip=clip, no_batches=no_batches,\n",
        "    epochs=epochs, lr_actor=lr_actor, lr_critic=lr_critic)\n",
        "\n",
        "no_games = 400 \n",
        "\n",
        "training = PpoLearning(no_games, ppo)\n",
        "training.train()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:  10 average score: -209.53796400402243 learning_iterations: 145\n",
            "Episode:  20 average score: -278.60772269850116 learning_iterations: 236\n",
            "Episode:  30 average score: -231.90745944621113 learning_iterations: 480\n",
            "Episode:  40 average score: -220.6486923304828 learning_iterations: 1214\n",
            "Episode:  50 average score: -247.24998869328593 learning_iterations: 1780\n",
            "Episode:  60 average score: -110.4599722604119 learning_iterations: 2390\n",
            "Episode:  70 average score: -122.00230300250897 learning_iterations: 3210\n",
            "Episode:  80 average score: -102.48884521698888 learning_iterations: 4051\n",
            "Episode:  90 average score: -107.05408009912017 learning_iterations: 5051\n",
            "Episode:  100 average score: -85.48473467150889 learning_iterations: 6051\n",
            "Episode:  110 average score: -32.89486626684405 learning_iterations: 7037\n",
            "Episode:  120 average score: -9.609541616276406 learning_iterations: 7865\n",
            "Episode:  130 average score: -4.804643463310162 learning_iterations: 8651\n",
            "Episode:  140 average score: 97.96652868860478 learning_iterations: 9491\n",
            "Episode:  150 average score: 77.8093922748115 learning_iterations: 9936\n",
            "Episode:  160 average score: 64.17285669640408 learning_iterations: 10321\n",
            "Episode:  170 average score: 76.34340533171464 learning_iterations: 10661\n",
            "Episode:  180 average score: 101.64066392509883 learning_iterations: 11355\n",
            "Episode:  190 average score: -88.8928821584868 learning_iterations: 12147\n",
            "Episode:  200 average score: 47.885412007191825 learning_iterations: 12856\n",
            "Episode:  210 average score: 211.12090722073563 learning_iterations: 13462\n",
            "Episode:  220 average score: 240.26228847220145 learning_iterations: 13901\n",
            "Episode:  230 average score: 127.11909100077219 learning_iterations: 14217\n",
            "Episode:  240 average score: 220.7689209527207 learning_iterations: 14591\n",
            "Episode:  250 average score: 101.90492493297893 learning_iterations: 15118\n",
            "Episode:  260 average score: 110.84379829837567 learning_iterations: 15685\n",
            "Episode:  270 average score: 122.20792715912384 learning_iterations: 16059\n",
            "Episode:  280 average score: 105.58227349897417 learning_iterations: 16633\n",
            "Episode:  290 average score: 217.64288445744933 learning_iterations: 17031\n",
            "Episode:  300 average score: 240.43914895607227 learning_iterations: 17392\n",
            "Episode:  310 average score: 201.46112664569887 learning_iterations: 17663\n",
            "Episode:  320 average score: 127.27244703986007 learning_iterations: 18044\n",
            "Episode:  330 average score: 180.8822658850539 learning_iterations: 18376\n",
            "Episode:  340 average score: 145.69922152627873 learning_iterations: 18756\n",
            "Episode:  350 average score: 148.203775654962 learning_iterations: 19132\n",
            "Episode:  360 average score: 167.22859778186654 learning_iterations: 19500\n",
            "Episode:  370 average score: 155.71561486977362 learning_iterations: 20018\n",
            "Episode:  380 average score: 219.7821289388359 learning_iterations: 20497\n",
            "Episode:  390 average score: 240.96189510893382 learning_iterations: 20847\n",
            "Episode:  400 average score: 231.21298478050258 learning_iterations: 21228\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}