{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PPO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkW8_A6Tl-PF",
        "outputId": "7117b278-d5ab-4efd-ea35-25fdce0d94de"
      },
      "source": [
        "!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcks6_Z0mUjs"
      },
      "source": [
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "from PPO import PPO\n",
        "\n",
        "class PpoLearning():\n",
        "\n",
        "    def __init__(self, no_games, PPO):\n",
        "\n",
        "        self.no_games = no_games\n",
        "        self.ppo = PPO\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        game_rew_hist = [] # list to hold the total rewards per game\n",
        "        iters = 0\n",
        "        step = 0\n",
        "\n",
        "        for game in range(1, no_games+1):\n",
        "\n",
        "            done = False\n",
        "            game_total_rew = 0\n",
        "            state = env.reset()\n",
        "\n",
        "            while not done:\n",
        "                action, log_prob, value = self.ppo.get_action_value(state)\n",
        "                next_state , reward, done, info = env.step(action)\n",
        "                step += 1\n",
        "                game_total_rew += reward\n",
        "                self.ppo.memory.save_memory(state, value, action, log_prob, reward, done)\n",
        "                if step % self.ppo.memory.T == 0:\n",
        "                    self.ppo.optimize()\n",
        "                    iters += 1\n",
        "                    self.ppo.memory.reset()\n",
        "                state = next_state\n",
        "\n",
        "            game_rew_hist.append(game_total_rew)\n",
        "\n",
        "            if (game) % 10 == 0:\n",
        "                avg_score = np.mean(game_rew_hist[-10:])\n",
        "\n",
        "                print('Episode: ', game, 'average score:', avg_score, \n",
        "                'learning_iterations:', iters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjBDBAOzmlBz",
        "outputId": "8e7392fd-3e50-4c9a-f957-925f562c34c4"
      },
      "source": [
        "# define the environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "# hyper-parameters for PPO learning\n",
        "epochs = 4 # number of epochs per learning\n",
        "no_batches = 2 # number of batches for splitting the timesteps\n",
        "hidden_dim = 256\n",
        "gamma = 0.99 # discount factor\n",
        "gae_lambda = 0.95 # gae smoothing parameter\n",
        "lr_actor = 0.0003\n",
        "lr_critic = 0.0003\n",
        "clip = 0.2 # PPO clipping epsilon parameter\n",
        "T = 10 # timesteps per each learning\n",
        "\n",
        "ppo = PPO(env=env, T=T, hidden_dim=hidden_dim, gamma=gamma, \n",
        "    gae_lambda=gae_lambda, clip=clip, no_batches=no_batches,\n",
        "    epochs=epochs, lr_actor=lr_actor, lr_critic=lr_critic)\n",
        "\n",
        "no_games = 400 \n",
        "\n",
        "training = PpoLearning(no_games, ppo)\n",
        "training.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode:  10 average score: -300.6651341053953 learning_iterations: 97\n",
            "Episode:  20 average score: -192.17584528859837 learning_iterations: 174\n",
            "Episode:  30 average score: -185.8861826264018 learning_iterations: 262\n",
            "Episode:  40 average score: -271.4786175337487 learning_iterations: 386\n",
            "Episode:  50 average score: -178.13296602210326 learning_iterations: 732\n",
            "Episode:  60 average score: -82.98857865063013 learning_iterations: 1348\n",
            "Episode:  70 average score: -156.58717081276703 learning_iterations: 2059\n",
            "Episode:  80 average score: -67.04186497424729 learning_iterations: 3034\n",
            "Episode:  90 average score: -81.21109393901874 learning_iterations: 3892\n",
            "Episode:  100 average score: -74.7623382420826 learning_iterations: 4756\n",
            "Episode:  110 average score: -78.46268327336813 learning_iterations: 5756\n",
            "Episode:  120 average score: -77.55109821764067 learning_iterations: 6734\n",
            "Episode:  130 average score: 184.26495201350303 learning_iterations: 7376\n",
            "Episode:  140 average score: 148.81720177237895 learning_iterations: 8084\n",
            "Episode:  150 average score: 124.92127632179836 learning_iterations: 8549\n",
            "Episode:  160 average score: 168.43003702623 learning_iterations: 9019\n",
            "Episode:  170 average score: 138.75962386564888 learning_iterations: 9649\n",
            "Episode:  180 average score: 177.65598566775967 learning_iterations: 10056\n",
            "Episode:  190 average score: 189.2420587911561 learning_iterations: 10542\n",
            "Episode:  200 average score: 95.56035837597037 learning_iterations: 10948\n",
            "Episode:  210 average score: 97.30683660647422 learning_iterations: 11494\n",
            "Episode:  220 average score: 39.66492020037451 learning_iterations: 12266\n",
            "Episode:  230 average score: 175.146923254937 learning_iterations: 12938\n",
            "Episode:  240 average score: 195.11601793036465 learning_iterations: 13468\n",
            "Episode:  250 average score: 118.06436179835025 learning_iterations: 14143\n",
            "Episode:  260 average score: 183.51308927185042 learning_iterations: 14534\n",
            "Episode:  270 average score: 60.42903814029627 learning_iterations: 15132\n",
            "Episode:  280 average score: 117.52941151817109 learning_iterations: 15798\n",
            "Episode:  290 average score: 158.97003544545285 learning_iterations: 16357\n",
            "Episode:  300 average score: 128.1172884102345 learning_iterations: 16901\n",
            "Episode:  310 average score: 107.9419217009541 learning_iterations: 17251\n",
            "Episode:  320 average score: 65.59006319724213 learning_iterations: 17786\n",
            "Episode:  330 average score: 224.63319875179727 learning_iterations: 18184\n",
            "Episode:  340 average score: 241.07372280182818 learning_iterations: 18542\n",
            "Episode:  350 average score: 223.0903998964947 learning_iterations: 18970\n",
            "Episode:  360 average score: 179.02793175043513 learning_iterations: 19269\n",
            "Episode:  370 average score: 260.56816233418647 learning_iterations: 19584\n",
            "Episode:  380 average score: 250.4042734444908 learning_iterations: 19912\n",
            "Episode:  390 average score: 93.26101377428367 learning_iterations: 20230\n",
            "Episode:  400 average score: 154.69384631137126 learning_iterations: 20813\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}