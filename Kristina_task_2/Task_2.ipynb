{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../Task_1\")\n",
    "import numpy as np\n",
    "import random\n",
    "from rooms import Rooms\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration(): \n",
    "        actions = ['up', 'down', 'left', 'right']\n",
    "        action = random.choice(actions)\n",
    "        num_action = actions.index(action)\n",
    "        return num_action, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploitation(q_list, state):\n",
    "    actions= ['up', 'down', 'left', 'right']\n",
    "    num_action = np.argmax(q_list)\n",
    "        \n",
    "    return num_action, actions[num_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(q_table, alpha, gamma, epsilon, epsilon_decay, game, rewards, epochs):\n",
    "    steps_taken = []\n",
    "    for epoch in range(epochs):\n",
    "        state = game.reset()\n",
    "        \n",
    "        actions_taken = []\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(steps):\n",
    "            random_num = random.uniform(0.1,1)\n",
    "               \n",
    "            if random_num < epsilon:\n",
    "                num_action, action = exploration()\n",
    "            else:\n",
    "                num_action, action = exploitation(q_table[state, :], state)\n",
    "                    \n",
    "            actions_taken.append(action)\n",
    "            next_cell = game.move([int(str(state)[0]),int(str(state)[1])], action)\n",
    "                    \n",
    "            new_state, step_reward, done = game.step(next_cell)\n",
    "                \n",
    "                \n",
    "            if q_table[state, num_action] == 0:\n",
    "                #When reweard is 0 in Q_table - new explored cell/state\n",
    "                q_table[state, num_action] = (1 - alpha) * q_table[state, num_action] + alpha * (step_reward + gamma * np.max(q_table[new_state, :]))\n",
    "            else:\n",
    "                q_table[state, num_action] = q_table[state, num_action] + alpha * (step_reward + gamma * np.max(q_table[new_state, :] - q_table[state, num_action]))\n",
    "            \n",
    "            \n",
    "            state = new_state\n",
    "            episode_reward += step_reward\n",
    "            \n",
    "            epsilon = epsilon * epsilon_decay\n",
    "            \n",
    "            if done == True:\n",
    "                steps_taken.append(step + 1)\n",
    "                break\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    return q_table, rewards, steps_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(q_table, game, episodes=3):\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = game.reset()\n",
    "        done = False\n",
    "        \n",
    "        print(f\"****** EPISODE {episode + 1} ******\\n\\n\\n\")\n",
    "        total_reward = 0\n",
    "        for step in range(steps):\n",
    "            game.display()\n",
    "            time.sleep(2)\n",
    "            \n",
    "            num_action = np.argmax(q_table[state,:])\n",
    "            actions = ['up', 'down', 'left', 'right']\n",
    "            action = actions[num_action]\n",
    "            \n",
    "            next_cell = game.move([int(str(state)[0]),int(str(state)[1])], action)\n",
    "            \n",
    "            \n",
    "            new_state, step_reward, done = game.step(next_cell)\n",
    "            total_reward += step_reward\n",
    "            \n",
    "            if done:\n",
    "                new_state = [int(str(new_state)[0]),int(str(new_state)[1])]\n",
    "                value = game.grid[new_state[0], new_state[1]]\n",
    "                \n",
    "                game.display()\n",
    "                if  step_reward >= 0:\n",
    "                    \n",
    "                    if value == 4:\n",
    "                        print(f\"\\tCONGRATULATIONS!!!\\n\\tYou have reached the Optimal Goal\\n\\tYour total reward is:{total_reward}\\t¦\\tIn {step+1} Steps\\n\\tWell Done!!\")\n",
    "                    else:\n",
    "                        print(f\"\\tWell Done!\\n\\tYou have reached the Sub-Optimal Goal\\n\\tYour total reward is:{total_reward}\\t¦\\tIn {step+1} Steps\\n\\tGood Job!!\")\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"\\tBAD NEWS!!!\\n\\tSadnly, you have Died\\n\\tYou either collided with a tornado or you ran out of time\\n\\tBetter Luck next time!\")\n",
    "                    time.sleep(3)\n",
    "                break\n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Environment\n",
    "game = Rooms(10)\n",
    "game.reset()\n",
    "\n",
    "#Create Q-table\n",
    "q_table = np.zeros((game.states.size, len(game.actions_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "turns = 10000\n",
    "steps = 500\n",
    "\n",
    "alpha = 0.6                        #learning rate\n",
    "gamma = 0.99                      #discount rate\n",
    "\n",
    "epsilon = 1                      #exploration_rate\n",
    "epsilon_decay= 0.99                #exploration_decay_rate\n",
    "\n",
    "rewards = []\n",
    "steps_taken= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table, rewards, steps_taken = q_learning(q_table, alpha, gamma, epsilon, epsilon_decay, game, rewards, turns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_hundred_episodes = np.split(np.array(rewards), turns/100)\n",
    "count = 100\n",
    "total_rewards = []\n",
    "\n",
    "for r in rewards_per_hundred_episodes:\n",
    "    total_rewards.append(sum(r/100))\n",
    "    count += 100\n",
    "    \n",
    "steps_per_hundred_episodes = np.split(np.array(steps_taken), turns/100)\n",
    "count = 100\n",
    "total_steps = []\n",
    "\n",
    "for r in steps_per_hundred_episodes:\n",
    "    total_steps.append(sum(r/100))\n",
    "    count += 100\n",
    "\n",
    "x = np.linspace(1, turns, 100)\n",
    "plt.plot(x,total_steps, label='Steps')\n",
    "plt.plot(x, total_rewards, label=\"Rewards\")\n",
    "plt.xlabel('Game', fontsize=20)\n",
    "plt.ylabel('Steps', fontsize=20)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(q_table, game, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
